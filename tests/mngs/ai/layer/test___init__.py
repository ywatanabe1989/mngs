#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-05-31 21:45:00 (ywatanabe)"
# File: test___init__.py
# --------------------------------------------------------------------------------

import pytest
import torch
import torch.nn as nn
from mngs.ai import layer


class TestLayerModuleInit:
    """Test suite for the layer module initialization and imports."""
    
    def test_module_imports(self):
        """Test that the layer module is importable."""
        assert layer is not None
        assert hasattr(layer, "__name__")
        assert layer.__name__ == "mngs.ai.layer"
    
    def test_pass_class_available(self):
        """Test that Pass class is available from the module."""
        assert hasattr(layer, "Pass")
        from mngs.ai.layer import Pass
        assert Pass is not None
        assert issubclass(Pass, nn.Module)
    
    def test_switch_function_available(self):
        """Test that switch function is available from the module."""
        assert hasattr(layer, "switch")
        from mngs.ai.layer import switch
        assert callable(switch)
    
    def test_direct_imports(self):
        """Test direct imports from the layer module."""
        from mngs.ai.layer import Pass, switch
        assert Pass is not None
        assert switch is not None
    
    def test_module_all_attribute(self):
        """Test that module defines __all__ properly (if exists)."""
        if hasattr(layer, "__all__"):
            assert isinstance(layer.__all__, list)
            assert "Pass" in layer.__all__
            assert "switch" in layer.__all__
    
    def test_pass_instantiation(self):
        """Test that Pass can be instantiated."""
        from mngs.ai.layer import Pass
        pass_layer = Pass()
        assert isinstance(pass_layer, Pass)
        assert isinstance(pass_layer, nn.Module)
    
    def test_switch_function_signature(self):
        """Test switch function signature and basic usage."""
        from mngs.ai.layer import switch
        import inspect
        sig = inspect.signature(switch)
        params = list(sig.parameters.keys())
        assert "layer" in params
        assert "is_used" in params
        assert len(params) == 2
    
    def test_module_docstring(self):
        """Test that the module has proper documentation."""
        # Module might not have docstring, but test the components
        from mngs.ai.layer import Pass, switch
        # Pass class should have documentation
        assert Pass.__module__ == "mngs.ai.layer._Pass"
        # switch function should have proper module reference
        assert switch.__module__ == "mngs.ai.layer._switch"
    
    def test_no_unexpected_exports(self):
        """Test that module doesn't export unexpected items."""
        from mngs.ai import layer
        public_attrs = [attr for attr in dir(layer) if not attr.startswith("_")]
        expected = {"Pass", "switch"}
        # There might be additional module attributes
        for attr in expected:
            assert attr in public_attrs
    
    def test_import_performance(self):
        """Test that imports are reasonably fast."""
        import time
        start = time.time()
        from mngs.ai.layer import Pass, switch
        elapsed = time.time() - start
        assert elapsed < 1.0  # Import should be fast (< 1 second)
    
    def test_circular_import_safety(self):
        """Test that there are no circular import issues."""
        # This would fail if there were circular imports
        from mngs.ai.layer import Pass, switch
        from mngs.ai.layer._Pass import Pass as PassDirect
        from mngs.ai.layer._switch import switch as switchDirect
        assert Pass is PassDirect
        assert switch is switchDirect
    
    def test_module_path(self):
        """Test that module has correct path attributes."""
        from mngs.ai import layer
        assert hasattr(layer, "__file__")
        assert hasattr(layer, "__package__")
        assert layer.__package__ == "mngs.ai.layer"
    
    def test_integration_with_torch(self):
        """Test basic integration with PyTorch."""
        from mngs.ai.layer import Pass
        pass_layer = Pass()
        x = torch.randn(10, 5)
        output = pass_layer(x)
        assert torch.equal(output, x)
    
    def test_switch_creates_pass_instance(self):
        """Test that switch creates Pass instances correctly."""
        from mngs.ai.layer import Pass, switch
        dummy_layer = nn.Linear(10, 5)
        switched_off = switch(dummy_layer, False)
        assert isinstance(switched_off, Pass)
    
    def test_namespace_pollution(self):
        """Test that importing doesn't pollute namespace."""
        import sys
        before_modules = set(sys.modules.keys())
        from mngs.ai.layer import Pass, switch
        after_modules = set(sys.modules.keys())
        new_modules = after_modules - before_modules
        # Should only add layer-related modules
        for mod in new_modules:
            assert "layer" in mod or "Pass" in mod or "switch" in mod


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Comprehensive tests for mngs.ai.layer module initialization."""

import pytest
import torch
import torch.nn as nn
import numpy as np
from unittest.mock import patch, Mock
import sys
import os

# Add the project root to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../../src')))


class TestLayerModuleImport:
    """Test suite for layer module import functionality."""
    
    def test_module_import(self):
        """Test that the module can be imported."""
        import mngs.ai.layer
        assert mngs.ai.layer is not None
        
    def test_module_has_file_attribute(self):
        """Test module has __file__ attribute."""
        import mngs.ai.layer
        assert hasattr(mngs.ai.layer, '__file__')
        assert 'layer/__init__.py' in mngs.ai.layer.__file__.replace('\\', '/')
        
    def test_parent_module_access(self):
        """Test layer module is accessible from parent."""
        import mngs.ai
        assert hasattr(mngs.ai, 'layer')
        
    def test_module_name(self):
        """Test module has correct name."""
        import mngs.ai.layer
        assert mngs.ai.layer.__name__ == 'mngs.ai.layer'


class TestPassLayer:
    """Test Pass layer availability and functionality."""
    
    def test_pass_import(self):
        """Test Pass can be imported from module."""
        from mngs.ai.layer import Pass
        assert Pass is not None
        assert issubclass(Pass, nn.Module)
        
    def test_pass_available_at_module_level(self):
        """Test Pass is available at module level."""
        import mngs.ai.layer
        assert hasattr(mngs.ai.layer, 'Pass')
        assert issubclass(mngs.ai.layer.Pass, nn.Module)
        
    def test_pass_instantiation(self):
        """Test Pass layer can be instantiated."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        assert isinstance(layer, nn.Module)
        assert isinstance(layer, Pass)
        
    def test_pass_forward(self):
        """Test Pass layer forward pass."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        x = torch.randn(10, 20)
        
        # Pass layer should return input unchanged
        output = layer(x)
        assert torch.equal(output, x)
        assert output is x  # Should be same object
        
    def test_pass_with_different_inputs(self):
        """Test Pass layer with various input types."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        
        # Test with different tensor shapes
        inputs = [
            torch.randn(5),
            torch.randn(10, 20),
            torch.randn(5, 10, 15),
            torch.randn(2, 3, 4, 5)
        ]
        
        for x in inputs:
            output = layer(x)
            assert torch.equal(output, x)
            assert output.shape == x.shape
            
    def test_pass_preserves_gradient(self):
        """Test Pass layer preserves gradient flow."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        x = torch.randn(10, 20, requires_grad=True)
        
        output = layer(x)
        assert output.requires_grad
        
        # Test gradient flow
        loss = output.sum()
        loss.backward()
        
        assert x.grad is not None
        assert torch.equal(x.grad, torch.ones_like(x))
        
    def test_pass_no_parameters(self):
        """Test Pass layer has no learnable parameters."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        params = list(layer.parameters())
        
        assert len(params) == 0
        
    def test_pass_device_compatibility(self):
        """Test Pass layer works on different devices."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        
        # CPU tensor
        x_cpu = torch.randn(10, 20)
        output_cpu = layer(x_cpu)
        assert output_cpu.device == x_cpu.device
        
        # CUDA tensor if available
        if torch.cuda.is_available():
            layer_cuda = layer.cuda()
            x_cuda = torch.randn(10, 20).cuda()
            output_cuda = layer_cuda(x_cuda)
            assert output_cuda.device == x_cuda.device


class TestSwitchFunction:
    """Test switch function availability and functionality."""
    
    def test_switch_import(self):
        """Test switch can be imported from module."""
        from mngs.ai.layer import switch
        assert switch is not None
        assert callable(switch)
        
    def test_switch_available_at_module_level(self):
        """Test switch is available at module level."""
        import mngs.ai.layer
        assert hasattr(mngs.ai.layer, 'switch')
        assert callable(mngs.ai.layer.switch)
        
    def test_switch_basic_functionality(self):
        """Test switch basic functionality."""
        from mngs.ai.layer import switch
        
        # Create test modules
        module1 = nn.Linear(10, 20)
        module2 = nn.Linear(10, 30)
        
        # Test switch with condition True
        x = torch.randn(5, 10)
        output_true = switch(True, module1, module2, x)
        expected_true = module1(x)
        assert torch.allclose(output_true, expected_true)
        
        # Test switch with condition False
        output_false = switch(False, module1, module2, x)
        expected_false = module2(x)
        assert torch.allclose(output_false, expected_false)
        
    def test_switch_with_different_modules(self):
        """Test switch with different module types."""
        from mngs.ai.layer import switch
        
        # Different module types
        relu = nn.ReLU()
        sigmoid = nn.Sigmoid()
        
        x = torch.randn(10, 20)
        
        # Switch between activations
        output_relu = switch(True, relu, sigmoid, x)
        assert torch.allclose(output_relu, relu(x))
        
        output_sigmoid = switch(False, relu, sigmoid, x)
        assert torch.allclose(output_sigmoid, sigmoid(x))
        
    def test_switch_preserves_gradients(self):
        """Test switch preserves gradient flow."""
        from mngs.ai.layer import switch
        
        module1 = nn.Linear(10, 20)
        module2 = nn.Linear(10, 20)
        
        x = torch.randn(5, 10, requires_grad=True)
        
        # Test with condition True
        output = switch(True, module1, module2, x)
        loss = output.sum()
        loss.backward()
        
        assert x.grad is not None
        assert module1.weight.grad is not None
        assert module2.weight.grad is None  # Should not have gradients
        
    def test_switch_with_boolean_tensor(self):
        """Test switch with boolean tensor condition."""
        from mngs.ai.layer import switch
        
        module1 = nn.Linear(10, 20)
        module2 = nn.Linear(10, 20)
        
        x = torch.randn(5, 10)
        
        # Boolean tensor conditions
        cond_tensor_true = torch.tensor(True)
        cond_tensor_false = torch.tensor(False)
        
        output_true = switch(cond_tensor_true, module1, module2, x)
        output_false = switch(cond_tensor_false, module1, module2, x)
        
        assert torch.allclose(output_true, module1(x))
        assert torch.allclose(output_false, module2(x))
        
    def test_switch_with_complex_modules(self):
        """Test switch with complex module architectures."""
        from mngs.ai.layer import switch
        
        # Complex modules
        branch1 = nn.Sequential(
            nn.Linear(10, 50),
            nn.ReLU(),
            nn.Linear(50, 20)
        )
        
        branch2 = nn.Sequential(
            nn.Linear(10, 30),
            nn.Tanh(),
            nn.Linear(30, 20)
        )
        
        x = torch.randn(8, 10)
        
        output1 = switch(True, branch1, branch2, x)
        output2 = switch(False, branch1, branch2, x)
        
        assert output1.shape == (8, 20)
        assert output2.shape == (8, 20)
        assert not torch.allclose(output1, output2)  # Different branches


class TestModuleExports:
    """Test module exports and namespace."""
    
    def test_public_exports(self):
        """Test that only expected items are exported."""
        import mngs.ai.layer
        
        public_attrs = [attr for attr in dir(mngs.ai.layer) if not attr.startswith('_')]
        
        # Should have Pass and switch as main exports
        assert 'Pass' in public_attrs
        assert 'switch' in public_attrs
        
    def test_no_private_exports(self):
        """Test private modules are not exposed."""
        import mngs.ai.layer
        
        # Private modules should not be accessible
        assert not hasattr(mngs.ai.layer, '_Pass')
        assert not hasattr(mngs.ai.layer, '_switch')
        
    def test_all_attribute(self):
        """Test __all__ attribute if present."""
        import mngs.ai.layer
        
        if hasattr(mngs.ai.layer, '__all__'):
            assert 'Pass' in mngs.ai.layer.__all__
            assert 'switch' in mngs.ai.layer.__all__


class TestImportPatterns:
    """Test various import patterns."""
    
    def test_import_star(self):
        """Test star import pattern."""
        namespace = {}
        exec("from mngs.ai.layer import *", namespace)
        
        # Pass and switch should be in namespace
        assert 'Pass' in namespace
        assert 'switch' in namespace
        
    def test_aliased_import(self):
        """Test aliased imports."""
        from mngs.ai.layer import Pass as PassThrough
        from mngs.ai.layer import switch as conditional_forward
        
        assert issubclass(PassThrough, nn.Module)
        assert callable(conditional_forward)
        
    def test_nested_import(self):
        """Test nested import pattern."""
        from mngs.ai import layer
        
        assert hasattr(layer, 'Pass')
        assert hasattr(layer, 'switch')
        
    def test_multiple_imports(self):
        """Test importing multiple items at once."""
        from mngs.ai.layer import Pass, switch
        
        assert issubclass(Pass, nn.Module)
        assert callable(switch)


class TestLayerIntegration:
    """Test integration with neural network architectures."""
    
    def test_pass_in_sequential(self):
        """Test Pass layer in nn.Sequential."""
        from mngs.ai.layer import Pass
        
        model = nn.Sequential(
            nn.Linear(10, 20),
            Pass(),
            nn.ReLU(),
            Pass(),
            nn.Linear(20, 10)
        )
        
        x = torch.randn(5, 10)
        output = model(x)
        
        assert output.shape == (5, 10)
        
    def test_switch_in_custom_module(self):
        """Test switch in custom module."""
        from mngs.ai.layer import switch
        
        class ConditionalNet(nn.Module):
            def __init__(self):
                super().__init__()
                self.branch1 = nn.Linear(10, 20)
                self.branch2 = nn.Linear(10, 20)
                
            def forward(self, x, use_branch1=True):
                return switch(use_branch1, self.branch1, self.branch2, x)
        
        model = ConditionalNet()
        x = torch.randn(5, 10)
        
        output1 = model(x, use_branch1=True)
        output2 = model(x, use_branch1=False)
        
        assert output1.shape == output2.shape == (5, 20)
        assert not torch.allclose(output1, output2)
        
    def test_pass_memory_efficiency(self):
        """Test Pass layer doesn't add memory overhead."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        
        # Check model size
        import sys
        layer_size = sys.getsizeof(layer)
        
        # Pass layer should be minimal
        assert layer_size < 10000  # Reasonable upper bound
        
        # No parameters
        param_count = sum(p.numel() for p in layer.parameters())
        assert param_count == 0


class TestErrorHandling:
    """Test error handling scenarios."""
    
    def test_switch_with_incompatible_modules(self):
        """Test switch with incompatible module outputs."""
        from mngs.ai.layer import switch
        
        # Modules with different output shapes
        module1 = nn.Linear(10, 20)
        module2 = nn.Linear(10, 30)  # Different output size
        
        x = torch.randn(5, 10)
        
        # Both should work, but produce different shapes
        output1 = switch(True, module1, module2, x)
        output2 = switch(False, module1, module2, x)
        
        assert output1.shape == (5, 20)
        assert output2.shape == (5, 30)
        
    def test_switch_with_none_modules(self):
        """Test switch with None modules."""
        from mngs.ai.layer import switch
        
        module = nn.Linear(10, 20)
        x = torch.randn(5, 10)
        
        # Should raise appropriate error
        with pytest.raises(Exception):
            switch(True, None, module, x)
            
        with pytest.raises(Exception):
            switch(False, module, None, x)
            
    def test_pass_with_non_tensor_input(self):
        """Test Pass layer with non-tensor inputs."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        
        # Should handle various inputs gracefully
        inputs = [
            [1, 2, 3],  # List
            np.array([1, 2, 3]),  # NumPy array
            5,  # Scalar
            "text",  # String
            {'key': 'value'}  # Dict
        ]
        
        for inp in inputs:
            output = layer(inp)
            assert output is inp  # Should return unchanged


class TestPerformance:
    """Test performance characteristics."""
    
    def test_pass_layer_speed(self):
        """Test Pass layer adds minimal overhead."""
        from mngs.ai.layer import Pass
        import time
        
        layer = Pass()
        x = torch.randn(100, 1000, 1000)
        
        # Time direct access
        start = time.time()
        _ = x
        direct_time = time.time() - start
        
        # Time through Pass layer
        start = time.time()
        _ = layer(x)
        pass_time = time.time() - start
        
        # Pass layer should add minimal overhead
        overhead = pass_time - direct_time
        assert overhead < 0.001  # Less than 1ms overhead
        
    def test_switch_performance(self):
        """Test switch function performance."""
        from mngs.ai.layer import switch
        import time
        
        module1 = nn.Identity()
        module2 = nn.Identity()
        x = torch.randn(100, 100)
        
        # Time many switches
        start = time.time()
        for i in range(1000):
            _ = switch(i % 2 == 0, module1, module2, x)
        switch_time = time.time() - start
        
        # Should be reasonably fast
        assert switch_time < 0.1  # 100ms for 1000 switches


class TestDocumentation:
    """Test module documentation."""
    
    def test_module_docstring(self):
        """Test module has docstring."""
        import mngs.ai.layer
        
        # Module might have docstring
        if hasattr(mngs.ai.layer, '__doc__'):
            if mngs.ai.layer.__doc__:
                assert isinstance(mngs.ai.layer.__doc__, str)
                
    def test_pass_class_docstring(self):
        """Test Pass class has docstring."""
        from mngs.ai.layer import Pass
        
        if hasattr(Pass, '__doc__'):
            assert Pass.__doc__ is None or isinstance(Pass.__doc__, str)
            
    def test_switch_function_docstring(self):
        """Test switch function has docstring."""
        from mngs.ai.layer import switch
        
        if hasattr(switch, '__doc__'):
            assert switch.__doc__ is None or isinstance(switch.__doc__, str)


class TestBackwardCompatibility:
    """Test backward compatibility concerns."""
    
    def test_legacy_import_patterns(self):
        """Test that legacy import patterns still work."""
        # Direct imports should work
        from mngs.ai.layer import Pass, switch
        assert issubclass(Pass, nn.Module)
        assert callable(switch)
        
        # Module import should work
        import mngs.ai.layer
        assert hasattr(mngs.ai.layer, 'Pass')
        assert hasattr(mngs.ai.layer, 'switch')
        
    def test_pass_layer_interface(self):
        """Test Pass layer maintains expected interface."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        
        # Should work like standard nn.Module
        assert hasattr(layer, 'forward')
        assert hasattr(layer, 'parameters')
        assert hasattr(layer, 'train')
        assert hasattr(layer, 'eval')


class TestEdgeCases:
    """Test edge cases and special scenarios."""
    
    def test_nested_pass_layers(self):
        """Test multiple nested Pass layers."""
        from mngs.ai.layer import Pass
        
        model = nn.Sequential(
            Pass(),
            Pass(),
            Pass()
        )
        
        x = torch.randn(10, 20)
        output = model(x)
        
        assert torch.equal(output, x)
        
    def test_switch_with_same_module(self):
        """Test switch with same module for both branches."""
        from mngs.ai.layer import switch
        
        module = nn.Linear(10, 20)
        x = torch.randn(5, 10)
        
        output_true = switch(True, module, module, x)
        output_false = switch(False, module, module, x)
        
        # Should produce same output regardless of condition
        assert torch.allclose(output_true, output_false)
        
    def test_pass_with_multiple_outputs(self):
        """Test Pass layer with functions returning multiple outputs."""
        from mngs.ai.layer import Pass
        
        layer = Pass()
        
        # Simulate multiple outputs (tuple)
        inputs = (torch.randn(10), torch.randn(20))
        outputs = layer(inputs)
        
        assert outputs is inputs
        assert len(outputs) == 2


if __name__ == "__main__":
<<<<<<< HEAD
    pytest.main([os.path.abspath(__file__), "-v"])
=======
    import os

    import pytest

    pytest.main([os.path.abspath(__file__)])

# --------------------------------------------------------------------------------
# Start of Source Code from: /data/gpfs/projects/punim2354/ywatanabe/mngs_repo/src/mngs/ai/layer/__init__.py
# --------------------------------------------------------------------------------
# from ._Pass import Pass
# from ._switch import switch

# --------------------------------------------------------------------------------
# End of Source Code from: /data/gpfs/projects/punim2354/ywatanabe/mngs_repo/src/mngs/ai/layer/__init__.py
# --------------------------------------------------------------------------------
>>>>>>> origin/main
