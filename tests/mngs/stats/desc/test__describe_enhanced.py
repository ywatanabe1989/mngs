#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-06-09 21:20:00 (ywatanabe)"
# File: /home/ywatanabe/proj/mngs_repo/tests/mngs/stats/desc/test__describe_enhanced.py
# ----------------------------------------
"""
Enhanced test suite for mngs.stats.desc.describe with advanced testing patterns.

This test suite demonstrates comprehensive testing for statistical functions,
including property-based testing, edge cases, performance benchmarks, and
statistical correctness verification.
"""

import os
import warnings
from unittest.mock import MagicMock, patch, PropertyMock

import numpy as np
import pandas as pd
import pytest
import torch
from hypothesis import given, strategies as st, assume, settings
from hypothesis.extra.numpy import arrays
from scipy import stats as scipy_stats

import mngs
from mngs.stats.desc._describe import describe, verify_non_leakage


# ----------------------------------------
# Fixtures
# ----------------------------------------

@pytest.fixture
def statistical_distributions():
    """Provide various statistical distributions for testing."""
    np.random.seed(42)
    n_samples = 1000
    
    return {
        'normal': np.random.normal(0, 1, n_samples),
        'uniform': np.random.uniform(-1, 1, n_samples),
        'exponential': np.random.exponential(1, n_samples),
        'chi_squared': np.random.chisquare(5, n_samples),
        'beta': np.random.beta(2, 5, n_samples),
        'gamma': np.random.gamma(2, 2, n_samples),
        'lognormal': np.random.lognormal(0, 1, n_samples),
        'poisson': np.random.poisson(5, n_samples).astype(float),
        'bimodal': np.concatenate([
            np.random.normal(-2, 0.5, n_samples//2),
            np.random.normal(2, 0.5, n_samples//2)
        ]),
        'heavy_tailed': np.random.standard_t(3, n_samples),
        'skewed': np.random.gamma(1, 2, n_samples),
    }


@pytest.fixture
def tensor_shapes():
    """Provide various tensor shapes for testing."""
    return {
        '1d': (100,),
        '2d': (50, 20),
        '3d': (10, 20, 30),
        '4d': (5, 10, 15, 20),
        '5d': (2, 5, 10, 15, 20),
        'batch_time_features': (32, 100, 64),  # Common ML shape
        'image': (16, 3, 224, 224),  # Batch of images
        'video': (8, 30, 3, 128, 128),  # Batch of videos
        'large': (1000, 1000),
        'tiny': (2, 2),
    }


@pytest.fixture
def edge_case_data():
    """Provide edge case data for testing."""
    return {
        'empty': np.array([]),
        'single': np.array([42.0]),
        'all_zeros': np.zeros(100),
        'all_ones': np.ones(100),
        'all_nan': np.full(100, np.nan),
        'half_nan': np.array([1, 2, np.nan, 4, np.nan]),
        'inf_values': np.array([1, np.inf, 3, -np.inf, 5]),
        'tiny_values': np.array([1e-308, 2e-308, 3e-308]),
        'huge_values': np.array([1e308, 2e308, 3e308]),
        'mixed_types': [1, 2.5, np.int64(3), np.float32(4)],
        'constant': np.full(100, 3.14),
        'alternating': np.array([1, -1] * 50),
        'monotonic': np.arange(100),
        'outliers': np.concatenate([np.ones(95), [100, -100, 1000, -1000, 10000]]),
    }


@pytest.fixture
def mock_torch_functions():
    """Mock torch statistical functions for isolation testing."""
    with patch('mngs.stats.desc._nan.nanmean') as mock_nanmean, \
         patch('mngs.stats.desc._nan.nanstd') as mock_nanstd, \
         patch('mngs.stats.desc._nan.nanmax') as mock_nanmax, \
         patch('mngs.stats.desc._nan.nanmin') as mock_nanmin:
        
        mock_nanmean.return_value = torch.tensor(0.0)
        mock_nanstd.return_value = torch.tensor(1.0)
        mock_nanmax.return_value = torch.tensor(10.0)
        mock_nanmin.return_value = torch.tensor(-10.0)
        
        yield {
            'nanmean': mock_nanmean,
            'nanstd': mock_nanstd,
            'nanmax': mock_nanmax,
            'nanmin': mock_nanmin,
        }


@pytest.fixture
def performance_benchmark():
    """Benchmark fixture for performance testing."""
    import time
    import tracemalloc
    
    class Benchmark:
        def __init__(self):
            self.results = {}
            
        def measure(self, name, func, *args, **kwargs):
            # Memory measurement
            tracemalloc.start()
            start_memory = tracemalloc.get_traced_memory()[0]
            
            # Time measurement
            start_time = time.perf_counter()
            result = func(*args, **kwargs)
            end_time = time.perf_counter()
            
            current_memory, peak_memory = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            self.results[name] = {
                'time': end_time - start_time,
                'memory': current_memory - start_memory,
                'peak_memory': peak_memory,
                'result': result
            }
            
            return result
            
        def compare(self, name1, name2):
            """Compare two benchmarks."""
            r1, r2 = self.results[name1], self.results[name2]
            return {
                'time_ratio': r1['time'] / r2['time'],
                'memory_ratio': r1['memory'] / r2['memory'],
            }
    
    return Benchmark()


# ----------------------------------------
# Wrapper Function Tests
# ----------------------------------------

class TestDescribeWrapper:
    """Test the high-level describe wrapper function."""
    
    def test_basic_functionality(self):
        """Test basic describe functionality."""
        data = [1, 2, 3, 4, 5]
        result = mngs.stats.describe(data)
        
        assert isinstance(result, dict)
        assert set(result.keys()) >= {'mean', 'std', 'min', 'max'}
        assert result['mean'] == 3.0
        assert result['min'] == 1.0
        assert result['max'] == 5.0
        assert result['std'] == pytest.approx(1.5811, rel=1e-4)
        
    @pytest.mark.parametrize("data_type", [list, tuple, np.array, pd.Series])
    def test_different_input_types(self, data_type):
        """Test with different input data types."""
        raw_data = [1, 2, 3, 4, 5]
        if data_type == pd.Series:
            data = pd.Series(raw_data)
        else:
            data = data_type(raw_data)
            
        result = mngs.stats.describe(data)
        assert result['mean'] == 3.0
        
    def test_with_pandas_dataframe(self):
        """Test with pandas DataFrame."""
        df = pd.DataFrame({
            'A': [1, 2, 3, 4, 5],
            'B': [2, 4, 6, 8, 10]
        })
        
        # Should work on flattened data
        result = mngs.stats.describe(df.values)
        assert result['mean'] == 4.5
        
    @pytest.mark.parametrize("distribution", [
        'normal', 'uniform', 'exponential', 'bimodal', 'skewed'
    ])
    def test_statistical_distributions(self, statistical_distributions, distribution):
        """Test with various statistical distributions."""
        data = statistical_distributions[distribution]
        result = mngs.stats.describe(data)
        
        # Verify results are reasonable
        assert 'mean' in result
        assert 'std' in result
        assert result['std'] >= 0
        assert result['min'] <= result['mean'] <= result['max']
        
        # Compare with scipy
        scipy_mean = np.mean(data)
        scipy_std = np.std(data, ddof=1)
        
        assert abs(result['mean'] - scipy_mean) < 0.01
        assert abs(result['std'] - scipy_std) < 0.01


# ----------------------------------------
# Internal Function Tests
# ----------------------------------------

class TestDescribeInternal:
    """Test the internal tensor-based describe function."""
    
    def test_basic_tensor_statistics(self):
        """Test basic statistics computation on tensors."""
        x = torch.randn(10, 20, 30)
        stats, names = describe(x, dim=-1)
        
        assert stats.shape == (10, 20, 7)  # 7 default statistics
        assert len(names) == 7
        assert all(isinstance(name, str) for name in names)
        
    @pytest.mark.parametrize("shape,dim,expected_shape", [
        ((10, 20), -1, (10, 7)),
        ((10, 20), 0, (20, 7)),
        ((5, 10, 15), (0, 1), (15, 7)),
        ((5, 10, 15), (-2, -1), (5, 7)),
        ((2, 3, 4, 5), (1, 2, 3), (2, 7)),
    ])
    def test_dimension_handling(self, shape, dim, expected_shape):
        """Test statistics computation along different dimensions."""
        x = torch.randn(*shape)
        stats, _ = describe(x, dim=dim, keepdims=False)
        assert stats.shape == expected_shape
        
    def test_keepdims_parameter(self):
        """Test keepdims functionality."""
        x = torch.randn(3, 4, 5)
        
        # keepdims=True
        stats_keep, _ = describe(x, dim=1, keepdims=True)
        assert stats_keep.shape == (3, 1, 5, 7)
        
        # keepdims=False
        stats_no_keep, _ = describe(x, dim=1, keepdims=False)
        assert stats_no_keep.shape == (3, 5, 7)
        
    @pytest.mark.parametrize("funcs,expected_count", [
        (["nanmean"], 1),
        (["nanmean", "nanstd"], 2),
        (["nanmean", "nanstd", "nanmax", "nanmin"], 4),
        ("all", None),  # Count varies but should be > 7
    ])
    def test_custom_functions(self, funcs, expected_count):
        """Test custom function selection."""
        x = torch.randn(5, 10)
        stats, names = describe(x, dim=-1, funcs=funcs)
        
        if expected_count is not None:
            assert stats.shape[-1] == expected_count
            assert len(names) == expected_count
        else:
            assert stats.shape[-1] > 7  # More than default
            assert len(names) == stats.shape[-1]
            
    def test_nan_handling(self):
        """Test handling of NaN values."""
        x = torch.tensor([
            [1.0, 2.0, float('nan'), 4.0, 5.0],
            [float('nan'), float('nan'), 3.0, 4.0, 5.0],
            [1.0, 2.0, 3.0, 4.0, 5.0]
        ])
        
        stats, names = describe(x, dim=-1, funcs=["nanmean", "nancount"])
        
        mean_idx = names.index("nanmean")
        count_idx = names.index("nancount")
        
        # Check NaN-aware mean
        assert torch.isclose(stats[0, mean_idx], torch.tensor(3.0))  # mean of [1,2,4,5]
        assert torch.isclose(stats[1, mean_idx], torch.tensor(4.0))  # mean of [3,4,5]
        assert torch.isclose(stats[2, mean_idx], torch.tensor(3.0))  # mean of [1,2,3,4,5]
        
        # Check counts
        assert stats[0, count_idx] == 4  # 4 non-NaN values
        assert stats[1, count_idx] == 3  # 3 non-NaN values
        assert stats[2, count_idx] == 5  # 5 non-NaN values


# ----------------------------------------
# Property-Based Testing
# ----------------------------------------

class TestPropertyBased:
    """Property-based tests using Hypothesis."""
    
    @given(
        data=arrays(
            dtype=np.float64,
            shape=st.integers(min_value=10, max_value=1000),
            elements=st.floats(min_value=-1e6, max_value=1e6, allow_nan=False)
        )
    )
    @settings(max_examples=50, deadline=5000)
    def test_wrapper_properties(self, data):
        """Test properties of the wrapper function."""
        result = mngs.stats.describe(data)
        
        # Properties that should always hold
        assert result['min'] <= result['mean'] <= result['max']
        assert result['std'] >= 0
        
        # If data has variation, std should be positive
        if len(np.unique(data)) > 1:
            assert result['std'] > 0
            
    @given(
        shape=st.tuples(
            st.integers(min_value=2, max_value=10),
            st.integers(min_value=2, max_value=20),
            st.integers(min_value=2, max_value=30)
        ),
        dim=st.integers(min_value=0, max_value=2)
    )
    @settings(max_examples=20, deadline=5000)
    def test_tensor_shape_consistency(self, shape, dim):
        """Test that output shapes are consistent."""
        x = torch.randn(*shape)
        stats, names = describe(x, dim=dim, keepdims=False)
        
        # Calculate expected shape
        expected_shape = list(shape)
        expected_shape.pop(dim)
        expected_shape.append(7)  # Default 7 statistics
        
        assert list(stats.shape) == expected_shape
        assert len(names) == 7
        
    @given(
        values=st.lists(
            st.floats(min_value=-1000, max_value=1000, allow_nan=False),
            min_size=5,
            max_size=100
        )
    )
    def test_statistical_correctness(self, values):
        """Test statistical correctness against known implementations."""
        data = np.array(values)
        result = mngs.stats.describe(data)
        
        # Compare with numpy/scipy
        np_mean = np.mean(data)
        np_std = np.std(data, ddof=1)  # Sample std
        np_min = np.min(data)
        np_max = np.max(data)
        
        assert abs(result['mean'] - np_mean) < 1e-6
        assert abs(result['std'] - np_std) < 1e-6
        assert abs(result['min'] - np_min) < 1e-6
        assert abs(result['max'] - np_max) < 1e-6


# ----------------------------------------
# Edge Cases and Error Handling
# ----------------------------------------

class TestEdgeCases:
    """Test edge cases and error conditions."""
    
    def test_empty_data(self):
        """Test with empty data."""
        result = mngs.stats.describe([])
        
        assert isinstance(result, dict)
        assert np.isnan(result['mean'])
        assert np.isnan(result['std'])
        
    def test_single_value(self):
        """Test with single value."""
        result = mngs.stats.describe([42.0])
        
        assert result['mean'] == 42.0
        assert result['min'] == 42.0
        assert result['max'] == 42.0
        assert result['std'] == 0.0
        
    def test_all_nan_values(self):
        """Test with all NaN values."""
        data = np.full(10, np.nan)
        result = mngs.stats.describe(data)
        
        assert np.isnan(result['mean'])
        assert np.isnan(result['std'])
        assert np.isnan(result['min'])
        assert np.isnan(result['max'])
        
    def test_inf_values(self):
        """Test with infinite values."""
        data = [1, 2, np.inf, 4, -np.inf]
        result = mngs.stats.describe(data)
        
        assert result['max'] == np.inf
        assert result['min'] == -np.inf
        assert np.isinf(result['mean'])
        
    def test_mixed_types(self, edge_case_data):
        """Test with mixed numeric types."""
        data = edge_case_data['mixed_types']
        result = mngs.stats.describe(data)
        
        # Should handle type conversion
        assert result['mean'] == 2.625
        assert result['min'] == 1.0
        assert result['max'] == 4.0
        
    @pytest.mark.parametrize("case", [
        'tiny_values', 'huge_values', 'constant', 'alternating', 'monotonic'
    ])
    def test_special_cases(self, edge_case_data, case):
        """Test various special cases."""
        data = edge_case_data[case]
        
        # Should not raise errors
        result = mngs.stats.describe(data)
        assert isinstance(result, dict)
        assert 'mean' in result
        
    def test_dtype_preservation(self):
        """Test that dtypes are handled correctly."""
        # Float32
        data_f32 = np.array([1, 2, 3], dtype=np.float32)
        result_f32 = mngs.stats.describe(data_f32)
        assert isinstance(result_f32['mean'], float)
        
        # Int64
        data_i64 = np.array([1, 2, 3], dtype=np.int64)
        result_i64 = mngs.stats.describe(data_i64)
        assert isinstance(result_i64['mean'], float)  # Stats are typically float


# ----------------------------------------
# Performance Tests
# ----------------------------------------

class TestPerformance:
    """Test performance characteristics."""
    
    def test_large_data_performance(self, performance_benchmark):
        """Test performance with large datasets."""
        sizes = [1000, 10000, 100000]
        
        for size in sizes:
            data = np.random.randn(size)
            
            # Benchmark describe
            result = performance_benchmark.measure(
                f'describe_{size}',
                mngs.stats.describe,
                data
            )
            
            # Benchmark numpy equivalent
            def numpy_stats(d):
                return {
                    'mean': np.mean(d),
                    'std': np.std(d, ddof=1),
                    'min': np.min(d),
                    'max': np.max(d)
                }
            
            np_result = performance_benchmark.measure(
                f'numpy_{size}',
                numpy_stats,
                data
            )
            
        # Compare performance
        for size in sizes:
            comparison = performance_benchmark.compare(
                f'describe_{size}',
                f'numpy_{size}'
            )
            
            # Should be within reasonable bounds (allowing for overhead)
            assert comparison['time_ratio'] < 10  # At most 10x slower
            
    def test_batch_processing_performance(self, performance_benchmark):
        """Test batch processing performance."""
        x = torch.randn(100, 50, 30)
        
        # Without batching
        result1 = performance_benchmark.measure(
            'no_batch',
            describe,
            x, dim=-1, batch_size=-1
        )
        
        # With batching
        result2 = performance_benchmark.measure(
            'batch_10',
            describe,
            x, dim=-1, batch_size=10
        )
        
        # Both should produce same results
        stats1, names1 = result1['result']
        stats2, names2 = result2['result']
        
        torch.testing.assert_close(stats1, stats2)
        assert names1 == names2
        
    def test_memory_efficiency(self, tensor_shapes):
        """Test memory usage with different tensor shapes."""
        import tracemalloc
        
        for name, shape in tensor_shapes.items():
            if np.prod(shape) > 1e7:  # Skip very large tensors
                continue
                
            x = torch.randn(*shape)
            
            tracemalloc.start()
            start_memory = tracemalloc.get_traced_memory()[0]
            
            stats, names = describe(x, dim=-1)
            
            current_memory, peak_memory = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            memory_used = current_memory - start_memory
            
            # Memory usage should be reasonable
            input_memory = x.element_size() * x.nelement()
            assert memory_used < input_memory * 5  # At most 5x input size


# ----------------------------------------
# Mock and Integration Tests
# ----------------------------------------

class TestMocking:
    """Tests using mocks to verify behavior."""
    
    def test_function_dispatch(self, mock_torch_functions):
        """Test that correct functions are called."""
        x = torch.randn(5, 10)
        
        stats, names = describe(x, dim=-1, funcs=["nanmean", "nanstd"])
        
        # Verify mocks were called
        mock_torch_functions['nanmean'].assert_called()
        mock_torch_functions['nanstd'].assert_called()
        
        # Verify they were called with correct arguments
        call_args = mock_torch_functions['nanmean'].call_args
        assert call_args[1]['dim'] == (-1,)
        assert 'keepdims' in call_args[1]
        
    @patch('torch.stack')
    def test_result_stacking(self, mock_stack):
        """Test that results are properly stacked."""
        x = torch.randn(5, 10)
        mock_stack.return_value = torch.zeros(5, 7)
        
        stats, names = describe(x, dim=-1)
        
        # Verify stack was called
        mock_stack.assert_called_once()
        assert mock_stack.call_args[1]['dim'] == -1
        
    def test_error_propagation(self):
        """Test that errors are properly propagated."""
        # Invalid dimension
        x = torch.randn(5, 10)
        with pytest.raises((IndexError, RuntimeError)):
            describe(x, dim=5)  # Invalid dimension
            
        # Invalid function name
        with pytest.raises(KeyError):
            describe(x, funcs=["invalid_function"])


# ----------------------------------------
# Statistical Correctness Tests
# ----------------------------------------

class TestStatisticalCorrectness:
    """Test statistical correctness of computations."""
    
    def test_known_distributions(self):
        """Test with distributions having known properties."""
        n_samples = 10000
        
        # Standard normal
        x = torch.randn(n_samples)
        stats, names = describe(x, dim=-1, funcs=["nanmean", "nanstd", "nanskewness", "nankurtosis"])
        
        mean_idx = names.index("nanmean")
        std_idx = names.index("nanstd")
        skew_idx = names.index("nanskewness")
        kurt_idx = names.index("nankurtosis")
        
        # Should be close to theoretical values
        assert abs(stats[mean_idx].item()) < 0.05  # Mean ≈ 0
        assert abs(stats[std_idx].item() - 1.0) < 0.05  # Std ≈ 1
        assert abs(stats[skew_idx].item()) < 0.1  # Skewness ≈ 0
        assert abs(stats[kurt_idx].item()) < 0.2  # Excess kurtosis ≈ 0
        
    def test_quantiles(self):
        """Test quantile calculations."""
        # Known data
        x = torch.arange(1, 101, dtype=torch.float32).unsqueeze(0)
        stats, names = describe(x, dim=-1, funcs=["nanq25", "nanq50", "nanq75"])
        
        q25_idx = names.index("nanq25")
        q50_idx = names.index("nanq50")
        q75_idx = names.index("nanq75")
        
        # Check quantiles
        assert abs(stats[0, q25_idx].item() - 25.5) < 1
        assert abs(stats[0, q50_idx].item() - 50.5) < 1
        assert abs(stats[0, q75_idx].item() - 75.5) < 1


# ----------------------------------------
# Non-Leakage Verification Tests
# ----------------------------------------

class TestNonLeakage:
    """Test the verify_non_leakage function."""
    
    def test_basic_non_leakage(self):
        """Test basic non-leakage verification."""
        x = torch.randn(10, 20, 30)
        
        # Should pass without error
        result = verify_non_leakage(x, dim=(1, 2))
        assert result is True
        
    @pytest.mark.parametrize("shape", [
        (5, 10, 15),
        (2, 3, 4, 5),
        (100, 50),
        (1, 10, 20),  # Single sample
    ])
    def test_different_shapes(self, shape):
        """Test non-leakage with different tensor shapes."""
        x = torch.randn(*shape)
        dim = tuple(range(1, len(shape)))  # All dims except first
        
        result = verify_non_leakage(x, dim=dim)
        assert result is True
        
    def test_with_special_values(self):
        """Test non-leakage with special values."""
        x = torch.randn(5, 10, 15)
        
        # Add NaN values
        x[0, 0, 0] = float('nan')
        x[2, 5, 7] = float('nan')
        
        # Add inf values
        x[1, 2, 3] = float('inf')
        x[3, 4, 5] = float('-inf')
        
        result = verify_non_leakage(x, dim=(1, 2))
        assert result is True


# ----------------------------------------
# Integration Tests
# ----------------------------------------

class TestIntegration:
    """Integration tests with real-world scenarios."""
    
    def test_ml_workflow(self):
        """Test in a typical ML workflow."""
        # Simulate batch of embeddings
        batch_size, seq_len, hidden_dim = 32, 128, 768
        embeddings = torch.randn(batch_size, seq_len, hidden_dim)
        
        # Compute statistics across sequence dimension
        stats, names = describe(embeddings, dim=1, keepdims=False)
        
        assert stats.shape == (batch_size, hidden_dim, 7)
        
        # Use statistics as features
        mean_idx = names.index("nanmean")
        std_idx = names.index("nanstd")
        
        features = torch.cat([
            stats[:, :, mean_idx],
            stats[:, :, std_idx]
        ], dim=-1)
        
        assert features.shape == (batch_size, hidden_dim * 2)
        
    def test_data_quality_check(self):
        """Test using describe for data quality checks."""
        # Simulate sensor data with potential issues
        sensors = torch.randn(100, 50)  # 100 time points, 50 sensors
        
        # Add some problematic values
        sensors[10:20, 5] = float('nan')  # Sensor 5 has missing data
        sensors[:, 10] = 0  # Sensor 10 is dead
        sensors[:, 15] = sensors[:, 15] * 1000  # Sensor 15 has scale issue
        
        # Analyze each sensor
        stats, names = describe(sensors, dim=0, funcs=["nanmean", "nanstd", "nancount"])
        
        mean_idx = names.index("nanmean")
        std_idx = names.index("nanstd")
        count_idx = names.index("nancount")
        
        # Detect issues
        dead_sensors = torch.where(stats[:, std_idx] < 1e-6)[0]
        assert 10 in dead_sensors
        
        missing_data_sensors = torch.where(stats[:, count_idx] < 100)[0]
        assert 5 in missing_data_sensors
        
        scale_issues = torch.where(torch.abs(stats[:, mean_idx]) > 10)[0]
        assert 15 in scale_issues


if __name__ == "__main__":
    pytest.main([__file__, "-v"])