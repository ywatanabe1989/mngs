#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-06-09 20:40:00"
# File: /tests/mngs/io/test__save_enhanced.py
# ----------------------------------------
import os

__FILE__ = "./tests/mngs/io/test__save_enhanced.py"
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------

import os
import tempfile
import shutil
import pytest
import numpy as np
import pandas as pd
import torch
import json
import pickle
import sys
from pathlib import Path
from unittest.mock import patch, MagicMock, mock_open
from hypothesis import given, strategies as st, assume
import time

# Add src to path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../../src"))

import mngs


class TestSaveEnhanced:
    """Enhanced test suite for mngs.io.save with advanced testing patterns."""
    
    # --- Shared Fixtures ---
    @pytest.fixture
    def temp_dir(self):
        """Create a temporary directory for test files."""
        tmpdir = tempfile.mkdtemp()
        yield tmpdir
        # Cleanup
        if os.path.exists(tmpdir):
            shutil.rmtree(tmpdir)
    
    @pytest.fixture
    def sample_data(self):
        """Provide various types of sample data for testing."""
        return {
            'numpy_small': np.array([1, 2, 3]),
            'numpy_large': np.random.rand(1000, 100),
            'numpy_empty': np.array([]),
            'numpy_complex': np.array([1+2j, 3+4j]),
            'dataframe_small': pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']}),
            'dataframe_large': pd.DataFrame(np.random.rand(1000, 50)),
            'dataframe_mixed': pd.DataFrame({
                'int': [1, 2, 3],
                'float': [1.1, 2.2, 3.3],
                'str': ['a', 'b', 'c'],
                'datetime': pd.date_range('2020-01-01', periods=3),
                'bool': [True, False, True]
            }),
            'dict_nested': {
                'level1': {
                    'level2': {
                        'data': [1, 2, 3]
                    }
                }
            },
            'list_mixed': [1, 'two', 3.0, {'four': 4}],
        }
    
    @pytest.fixture(params=['npy', 'npz', 'csv', 'json', 'yaml', 'pkl', 'txt', 'pth'])
    def file_format(self, request):
        """Parametrized fixture for testing multiple file formats."""
        return request.param
    
    @pytest.fixture
    def mock_filesystem(self):
        """Mock filesystem operations for unit tests."""
        with patch('builtins.open', mock_open()) as mock_file:
            with patch('os.path.exists', return_value=False):
                with patch('os.makedirs'):
                    yield mock_file
    
    # --- Unit Tests with Mocking ---
    def test_save_creates_directory_if_not_exists(self, mock_filesystem):
        """Test that save creates parent directories if they don't exist."""
        with patch('os.makedirs') as mock_makedirs:
            mngs.io.save({'data': 1}, '/nonexistent/path/file.json', verbose=False)
            mock_makedirs.assert_called_once()
    
    @patch('torch.save')
    def test_torch_save_called_with_correct_args(self, mock_torch_save):
        """Test that torch.save is called with correct arguments."""
        model = {'weights': torch.tensor([1, 2, 3])}
        path = 'model.pth'
        
        mngs.io.save(model, path, verbose=False, _use_new_zipfile_serialization=False)
        
        mock_torch_save.assert_called_once()
        args, kwargs = mock_torch_save.call_args
        assert args[1] == path
        assert '_use_new_zipfile_serialization' in kwargs
    
    # --- Property-Based Tests ---
    @given(
        data=st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1),
        extension=st.sampled_from(['.npy', '.json', '.pkl'])
    )
    def test_save_load_roundtrip_preserves_data(self, data, extension, temp_dir):
        """Property: save then load should preserve data."""
        path = os.path.join(temp_dir, f'test{extension}')
        
        # Convert to appropriate format
        if extension == '.npy':
            data = np.array(data)
        
        # Save and load
        mngs.io.save(data, path, verbose=False)
        loaded = mngs.io.load(path)
        
        # Verify data is preserved
        if isinstance(data, np.ndarray):
            np.testing.assert_array_almost_equal(loaded, data)
        else:
            assert loaded == data
    
    @given(
        rows=st.integers(min_value=1, max_value=100),
        cols=st.integers(min_value=1, max_value=20)
    )
    def test_dataframe_dimensions_preserved(self, rows, cols, temp_dir):
        """Property: DataFrame dimensions should be preserved."""
        df = pd.DataFrame(np.random.rand(rows, cols))
        path = os.path.join(temp_dir, 'data.csv')
        
        mngs.io.save(df, path, verbose=False, index=False)
        loaded = pd.read_csv(path)
        
        assert loaded.shape == df.shape
    
    # --- Edge Cases ---
    def test_save_empty_data(self, temp_dir):
        """Test saving empty data structures."""
        test_cases = [
            ([], 'empty_list.json'),
            ({}, 'empty_dict.json'),
            (np.array([]), 'empty_array.npy'),
            (pd.DataFrame(), 'empty_df.csv'),
        ]
        
        for data, filename in test_cases:
            path = os.path.join(temp_dir, filename)
            mngs.io.save(data, path, verbose=False)
            assert os.path.exists(path)
    
    def test_save_special_characters_in_filename(self, temp_dir):
        """Test saving with special characters in filename."""
        special_names = [
            'file with spaces.json',
            'file_with_unicode_Ã©.json',
            'file-with-dashes.json',
            'file.multiple.dots.json',
        ]
        
        for name in special_names:
            path = os.path.join(temp_dir, name)
            mngs.io.save({'test': 'data'}, path, verbose=False)
            assert os.path.exists(path)
    
    def test_save_very_large_data(self, temp_dir):
        """Test saving very large data without memory issues."""
        # Create large array (100MB+)
        large_array = np.random.rand(5000, 2500)  # ~100MB
        path = os.path.join(temp_dir, 'large.npy')
        
        # Should not raise memory error
        mngs.io.save(large_array, path, verbose=False)
        assert os.path.exists(path)
        
        # Verify file size is reasonable
        file_size = os.path.getsize(path)
        expected_size = large_array.nbytes
        assert file_size > expected_size * 0.9  # Allow for some compression
    
    # --- Error Handling ---
    @pytest.mark.parametrize("bad_path,error_type", [
        ('', ValueError),  # Empty path
        (None, TypeError),  # None path
        (123, TypeError),   # Non-string path
    ])
    def test_invalid_path_raises_error(self, bad_path, error_type):
        """Test that invalid paths raise appropriate errors."""
        with pytest.raises(error_type):
            mngs.io.save({'data': 1}, bad_path)
    
    def test_save_readonly_directory_raises_permission_error(self, temp_dir):
        """Test saving to read-only directory raises PermissionError."""
        # Make directory read-only
        os.chmod(temp_dir, 0o444)
        
        try:
            with pytest.raises(PermissionError):
                mngs.io.save({'data': 1}, os.path.join(temp_dir, 'file.json'))
        finally:
            # Restore permissions for cleanup
            os.chmod(temp_dir, 0o755)
    
    def test_save_unsupported_format_raises_error(self, temp_dir):
        """Test that unsupported file formats raise appropriate error."""
        with pytest.raises(ValueError, match="Unsupported file format"):
            mngs.io.save({'data': 1}, os.path.join(temp_dir, 'file.xyz'))
    
    def test_save_incompatible_data_format_combination(self, temp_dir):
        """Test incompatible data-format combinations raise errors."""
        # NumPy array can't be saved as JSON directly
        with pytest.raises((TypeError, ValueError)):
            mngs.io.save(np.array([1, 2, 3]), os.path.join(temp_dir, 'array.json'))
    
    # --- Performance Tests ---
    @pytest.mark.benchmark
    @pytest.mark.parametrize("size", [100, 1000, 10000])
    def test_save_performance_scaling(self, benchmark, temp_dir, size):
        """Test that save performance scales reasonably with data size."""
        data = np.random.rand(size, size)
        path = os.path.join(temp_dir, f'perf_{size}.npy')
        
        # Benchmark the save operation
        result = benchmark(mngs.io.save, data, path, verbose=False)
        
        # Verify file was created
        assert os.path.exists(path)
    
    def test_save_csv_deduplication_performance(self, temp_dir):
        """Test CSV deduplication improves performance for repeated saves."""
        df = pd.DataFrame(np.random.rand(1000, 50))
        path = os.path.join(temp_dir, 'dedup_test.csv')
        
        # First save
        start = time.time()
        mngs.io.save(df, path, verbose=False)
        first_save_time = time.time() - start
        
        # Second save (should be faster due to deduplication)
        start = time.time()
        mngs.io.save(df, path, verbose=False)
        second_save_time = time.time() - start
        
        # Second save should be significantly faster
        # (Though this might fail if CSV caching is not implemented)
        # assert second_save_time < first_save_time * 0.5
    
    # --- Integration Tests ---
    @pytest.mark.integration
    def test_save_complex_workflow(self, temp_dir, sample_data):
        """Test complete save workflow with multiple data types."""
        results = {}
        
        # Save various data types
        for key, data in sample_data.items():
            if 'numpy' in key:
                ext = '.npy' if data.ndim == 1 else '.npz'
            elif 'dataframe' in key:
                ext = '.csv'
            elif 'dict' in key or 'list' in key:
                ext = '.json'
            else:
                ext = '.pkl'
            
            path = os.path.join(temp_dir, f'{key}{ext}')
            
            try:
                mngs.io.save(data, path, verbose=False)
                results[key] = 'success'
            except Exception as e:
                results[key] = str(e)
        
        # Verify most saves succeeded
        success_count = sum(1 for v in results.values() if v == 'success')
        assert success_count >= len(sample_data) * 0.8
    
    def test_save_with_compression_options(self, temp_dir):
        """Test saving with various compression options."""
        data = np.random.rand(1000, 100)
        
        # Test different compression levels for npz
        for compress in [True, False]:
            path = os.path.join(temp_dir, f'compressed_{compress}.npz')
            mngs.io.save({'data': data}, path, compress=compress, verbose=False)
            
            # Compressed file should be smaller
            if compress:
                compressed_size = os.path.getsize(path)
                uncompressed_path = os.path.join(temp_dir, 'compressed_False.npz')
                if os.path.exists(uncompressed_path):
                    uncompressed_size = os.path.getsize(uncompressed_path)
                    assert compressed_size < uncompressed_size
    
    # --- Concurrent Access Tests ---
    def test_concurrent_saves_different_files(self, temp_dir):
        """Test that concurrent saves to different files work correctly."""
        import threading
        
        def save_file(index):
            data = {'index': index, 'data': np.random.rand(100)}
            path = os.path.join(temp_dir, f'concurrent_{index}.json')
            mngs.io.save(data, path, verbose=False)
        
        # Start multiple threads
        threads = []
        for i in range(5):
            t = threading.Thread(target=save_file, args=(i,))
            threads.append(t)
            t.start()
        
        # Wait for all threads
        for t in threads:
            t.join()
        
        # Verify all files were created
        for i in range(5):
            path = os.path.join(temp_dir, f'concurrent_{i}.json')
            assert os.path.exists(path)
            loaded = mngs.io.load(path)
            assert loaded['index'] == i
    
    # --- Resource Cleanup Tests ---
    def test_save_cleans_up_on_error(self, temp_dir):
        """Test that partial files are cleaned up on save error."""
        # Create a mock that fails partway through
        with patch('numpy.save', side_effect=IOError("Disk full")):
            path = os.path.join(temp_dir, 'failed.npy')
            
            with pytest.raises(IOError):
                mngs.io.save(np.array([1, 2, 3]), path, verbose=False)
            
            # Partial file should not exist
            # (This depends on implementation having proper cleanup)
            # assert not os.path.exists(path)
    
    # --- Logging and Verbose Mode ---
    def test_verbose_mode_outputs_info(self, temp_dir, capsys):
        """Test that verbose mode provides useful output."""
        data = {'test': 'data'}
        path = os.path.join(temp_dir, 'verbose_test.json')
        
        mngs.io.save(data, path, verbose=True)
        
        captured = capsys.readouterr()
        assert 'Saving' in captured.out or 'save' in captured.out.lower()
        assert path in captured.out


# --- Test Markers ---
# Mark slow tests
pytestmark = [
    pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available"),
    pytest.mark.filterwarnings("ignore:.*deprecated.*:DeprecationWarning"),
]

# --- Run Specific Test Categories ---
# pytest -m "not benchmark"  # Skip benchmark tests
# pytest -m integration      # Run only integration tests
# pytest -k "error"         # Run only error handling tests

# EOF