# Test Coverage Analysis

## Overview

This document provides an analysis of the current test coverage in the MNGS codebase, identifying patterns and areas needing improvement.

## Current State

1. The codebase contains a large number of test files (5000+) that follow a consistent structure.
2. However, many of these files appear to be empty templates with:
   - Header information
   - A main block for running with pytest
   - The source code of the module they should test (commented out)
   - But no actual test implementation

3. This suggests the project has a good scaffolding for test-driven development, but many modules lack actual test implementations.

## Examples

### Empty Test Template Example

```python
# File: tests/mngs/ai/_gen_ai/test__BaseGenAI.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Timestamp: "2025-05-03 11:59:38 (ywatanabe)"
# File: /home/ywatanabe/proj/mngs_repo/tests/mngs/ai/_gen_ai/test__BaseGenAI.py
# ----------------------------------------
import os
__FILE__ = (
    "./tests/mngs/ai/_gen_ai/test__BaseGenAI.py"
)
__DIR__ = os.path.dirname(__FILE__)
# ----------------------------------------
# Add your tests here

if __name__ == "__main__":
    import os

    import pytest

    pytest.main([os.path.abspath(__file__)])

# [Source code commented out]
```

### Well-Implemented Test Example

Our recent implementation of `test__save.py` serves as a good example of a properly implemented test file:

```python
def test_torch_save_pt_extension():
    """Test that PyTorch models can be saved with .pt extension."""
    from mngs.io._save import _save

    # Create temp file path
    with tempfile.NamedTemporaryFile(suffix=".pt", delete=False) as tmp:
        temp_path = tmp.name

    try:
        # Create simple model tensor
        model = torch.tensor([1, 2, 3])
        
        # Test saving with .pt extension
        _save(model, temp_path, verbose=False)
        
        # Verify the file exists and can be loaded back
        assert os.path.exists(temp_path)
        loaded_model = torch.load(temp_path)
        assert torch.all(loaded_model == model)
    finally:
        # Clean up
        if os.path.exists(temp_path):
            os.unlink(temp_path)
```

## Proposed Approach

Based on this analysis, the most effective approach to improve test coverage would be:

1. Focus on high-priority modules first
   - Core functionality modules
   - Modules with complex logic
   - Modules that are frequently used by other parts of the codebase

2. Leverage existing test templates
   - Use the existing scaffolding
   - Implement proper test cases using pytest
   - Follow the pattern established in our `test__save.py` implementation

3. Prioritize modules where tests can be implemented without complex mocking
   - Utils and helpers
   - Low-level functions
   - Pure functions with clear inputs and outputs

## Next Steps

1. Identify 3-5 high-priority modules for initial implementation
2. Implement comprehensive tests for these modules 
3. Document patterns and best practices for future test implementation

## High Priority Modules (Initial Assessment)

Based on file structure and naming, these modules may be good candidates for initial focus:

1. Core utility modules:
   - `mngs/str/*` - String manipulation utilities
   - `mngs/dict/*` - Dictionary handling utilities
   - `mngs/path/*` - Path handling utilities

2. Data handling modules:
   - `mngs/pd/*` - Pandas utilities 
   - `mngs/io/*` - Input/Output utilities

3. Math/Stats modules:
   - `mngs/linalg/*` - Linear algebra utilities
   - `mngs/stats/*` - Statistical functions